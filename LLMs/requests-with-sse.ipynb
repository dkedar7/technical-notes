{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Server-Sent Events (SSE) with Python\"\n",
    "author: \"Kedar Dabhadkar\"\n",
    "date: 2026-01-03\n",
    "description: \"Serving and consuming SSE streams for real-time LLM responses\"\n",
    "type: technical_note\n",
    "categories: [\"Python\", \"LLM\", \"FastAPI\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "SSE (Server-Sent Events) is how LLM APIs stream responses token-by-token over HTTP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Serving SSE with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: list\n",
    "    stream: bool = True\n",
    "\n",
    "async def generate_response(messages: list):\n",
    "    \"\"\"Simulate LLM token streaming.\"\"\"\n",
    "    response_text = \"Hello! This is a streamed response.\"\n",
    "    for token in response_text.split():\n",
    "        chunk = {\"choices\": [{\"delta\": {\"content\": token + \" \"}}]}\n",
    "        yield f\"data: {json.dumps(chunk)}\\n\\n\"\n",
    "        await asyncio.sleep(0.1)\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(request: ChatRequest):\n",
    "    return StreamingResponse(\n",
    "        generate_response(request.messages),\n",
    "        media_type=\"text/event-stream\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Consuming SSE with Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def consume_sse(url, headers=None, payload=None):\n",
    "    \"\"\"Basic SSE consumer.\"\"\"\n",
    "    response = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            decoded = line.decode('utf-8')\n",
    "            if decoded.startswith('data: '):\n",
    "                data = decoded[6:]\n",
    "                if data == '[DONE]':\n",
    "                    break\n",
    "                yield json.loads(data)\n",
    "\n",
    "# Usage with OpenAI-compatible API\n",
    "def stream_chat(api_url, api_key, messages):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    payload = {\"messages\": messages, \"stream\": True}\n",
    "    \n",
    "    for chunk in consume_sse(api_url, headers, payload):\n",
    "        content = chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')\n",
    "        if content:\n",
    "            print(content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "- **Server**: Use `StreamingResponse` with `media_type=\"text/event-stream\"`\n",
    "- **Client**: Use `stream=True` and `iter_lines()`\n",
    "- **Format**: `data: {json}\\n\\n` (double newline separates events)\n",
    "- **End signal**: `[DONE]` indicates stream completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
