{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"FastAPI + Long Polling for Streaming\"\n",
    "date: 2026-01-04\n",
    "categories: [Engineering, Python]\n",
    "description: \"A simple pattern for streaming responses using FastAPI and long polling\"\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Long Polling?\n",
    "\n",
    "When you need to stream data from a backend to a frontend but can't use WebSockets (e.g., certain deployment constraints, simpler client implementation), **long polling** is a reliable alternative.\n",
    "\n",
    "The pattern:\n",
    "1. Client makes a request\n",
    "2. Server holds the connection open until data is available\n",
    "3. Server sends response, client immediately reconnects\n",
    "4. Repeat until streaming is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "async def generate_stream() -> AsyncGenerator[str, None]:\n",
    "    \"\"\"Simulate streaming data (e.g., LLM tokens).\"\"\"\n",
    "    chunks = [\"Hello \", \"world! \", \"This \", \"is \", \"streaming \", \"data.\"]\n",
    "    for chunk in chunks:\n",
    "        await asyncio.sleep(0.3)  # Simulate delay\n",
    "        yield chunk\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "async def stream_endpoint():\n",
    "    return StreamingResponse(\n",
    "        generate_stream(),\n",
    "        media_type=\"text/plain\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Polling with Session State\n",
    "\n",
    "For more complex scenarios where you need to track progress across requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uuid\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# In-memory session storage (use Redis in production)\n",
    "sessions: dict[str, dict] = {}\n",
    "\n",
    "class StartResponse(BaseModel):\n",
    "    session_id: str\n",
    "\n",
    "class PollResponse(BaseModel):\n",
    "    data: str\n",
    "    done: bool\n",
    "\n",
    "@app.post(\"/start\")\n",
    "async def start_stream() -> StartResponse:\n",
    "    \"\"\"Initialize a streaming session.\"\"\"\n",
    "    session_id = str(uuid.uuid4())\n",
    "    sessions[session_id] = {\n",
    "        \"queue\": asyncio.Queue(),\n",
    "        \"done\": False\n",
    "    }\n",
    "    # Start background task to produce data\n",
    "    asyncio.create_task(produce_data(session_id))\n",
    "    return StartResponse(session_id=session_id)\n",
    "\n",
    "async def produce_data(session_id: str):\n",
    "    \"\"\"Background task that produces streaming data.\"\"\"\n",
    "    chunks = [\"Processing \", \"your \", \"request \", \"now...\"]\n",
    "    for chunk in chunks:\n",
    "        await asyncio.sleep(0.5)\n",
    "        await sessions[session_id][\"queue\"].put(chunk)\n",
    "    sessions[session_id][\"done\"] = True\n",
    "\n",
    "@app.get(\"/poll/{session_id}\")\n",
    "async def poll(session_id: str) -> PollResponse:\n",
    "    \"\"\"Long poll for next chunk of data.\"\"\"\n",
    "    if session_id not in sessions:\n",
    "        raise HTTPException(status_code=404, detail=\"Session not found\")\n",
    "    \n",
    "    session = sessions[session_id]\n",
    "    \n",
    "    try:\n",
    "        # Wait up to 30s for data\n",
    "        data = await asyncio.wait_for(\n",
    "            session[\"queue\"].get(),\n",
    "            timeout=30.0\n",
    "        )\n",
    "        return PollResponse(data=data, done=False)\n",
    "    except asyncio.TimeoutError:\n",
    "        # Return empty response, client should retry\n",
    "        return PollResponse(data=\"\", done=session[\"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Implementation (JavaScript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "async function streamWithLongPolling() {\n",
    "    // Start session\n",
    "    const startRes = await fetch('/start', { method: 'POST' });\n",
    "    const { session_id } = await startRes.json();\n",
    "    \n",
    "    // Poll until done\n",
    "    let done = false;\n",
    "    while (!done) {\n",
    "        const pollRes = await fetch(`/poll/${session_id}`);\n",
    "        const result = await pollRes.json();\n",
    "        \n",
    "        if (result.data) {\n",
    "            console.log('Received:', result.data);\n",
    "            // Update UI with streamed data\n",
    "        }\n",
    "        done = result.done;\n",
    "    }\n",
    "    console.log('Stream complete');\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Considerations\n",
    "\n",
    "| Aspect | Recommendation |\n",
    "|--------|----------------|\n",
    "| Timeout | Set reasonable poll timeout (30s typical) |\n",
    "| Session cleanup | Add TTL-based cleanup for abandoned sessions |\n",
    "| Production storage | Use Redis instead of in-memory dict |\n",
    "| Error handling | Client should retry on network errors |\n",
    "| Scaling | Session affinity or shared state needed for multiple workers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use What\n",
    "\n",
    "- **SSE (Server-Sent Events)**: Best for one-way server-to-client streaming\n",
    "- **WebSockets**: Bidirectional, real-time communication\n",
    "- **Long Polling**: When SSE/WebSockets aren't available, or for simpler deployments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
