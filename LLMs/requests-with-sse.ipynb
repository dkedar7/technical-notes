{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Server-Sent Events (SSE) with Python\"\n",
    "author: \"Kedar Dabhadkar\"\n",
    "date: 2026-01-03\n",
    "description: \"Serving and consuming SSE streams for real-time LLM responses\"\n",
    "type: technical_note\n",
    "categories: [\"Python\", \"LLM\", \"FastAPI\"]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "SSE (Server-Sent Events) is how LLM APIs stream responses token-by-token. Format: `data: {json}\\n\\n` with `[DONE]` signaling completion.\n",
    "\n",
    "#### Part 1: Serving with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "import asyncio, json\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: list\n",
    "    stream: bool = True\n",
    "\n",
    "async def generate_response(messages: list):\n",
    "    for token in \"Hello! This is streamed.\".split():\n",
    "        yield f\"data: {json.dumps({'choices': [{'delta': {'content': token + ' '}}]})}\\n\\n\"\n",
    "        await asyncio.sleep(0.1)\n",
    "    yield \"data: [DONE]\\n\\n\"\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat(req: ChatRequest):\n",
    "    return StreamingResponse(generate_response(req.messages), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "#### Part 2: Consuming with Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "\n",
    "def consume_sse(url, headers=None, payload=None):\n",
    "    resp = requests.post(url, headers=headers, json=payload, stream=True)\n",
    "    resp.raise_for_status()\n",
    "    for line in resp.iter_lines():\n",
    "        if line and (decoded := line.decode('utf-8')).startswith('data: '):\n",
    "            if (data := decoded[6:]) == '[DONE]': break\n",
    "            yield json.loads(data)\n",
    "\n",
    "# Usage: stream from OpenAI-compatible API\n",
    "def stream_chat(url, key, messages):\n",
    "    for chunk in consume_sse(url, {\"Authorization\": f\"Bearer {key}\"}, {\"messages\": messages, \"stream\": True}):\n",
    "        if content := chunk.get('choices', [{}])[0].get('delta', {}).get('content'):\n",
    "            print(content, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
